{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import xmltodict\n",
    "import base64\n",
    "import struct\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "# import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchvision import datasets, models, transforms\n",
    "# from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import cv2\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from skimage import io\n",
    "import copy\n",
    "from torch import optim, cuda\n",
    "import glob\n",
    "from collections import Counter\n",
    "# Useful for examining network\n",
    "from functools import reduce\n",
    "from operator import __add__\n",
    "# from torchsummary import summary\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Useful for examining network\n",
    "from functools import reduce\n",
    "from operator import __add__\n",
    "# from torchsummary import summary\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Image manipulations\n",
    "from PIL import Image\n",
    "\n",
    "# Timing utility\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Baseline wander removal\n",
    "import scipy\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert XML files into .npy arrays (user changes XML_input_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "XML_input_directory_path = '/insert/path/to/XML/files' #this should be changed\n",
    "# XML_input_directory_path = '/Users/pae2/Box/Data/muse_XML_waveform_miniset/' #this should be changed\n",
    "npy_output_directory_path = os.path.join(home_dir,'ekg_waveform_output') #this can be left as default\n",
    "query_metadata_result = '/path/to/query_metadata_result.csv' #this should be changed \n",
    "# query_metadata_result = '/Users/pae2/Box/Data/MuseLabelGeneration/Master_File_MUSE_Pt_List.csv'\n",
    "fpath_eval_df_newest_ecg_waveform_data_prebuilt = 'path/to/prebuilt_array.npy' #stacked waveform data of newest ecg per pt, will build below as [X,2500,12,1]\n",
    "fpath_eval_df_newest_ecg_waveform_data = os.path.join(home_dir,'eval_newest_ecg_per_pt_waveform_features.npy') #this can be left as default\n",
    "fpath_eval_df_newest_ecg_waveform_data_wander_removed = os.path.join(home_dir,'eval_newest_ecg_per_pt_wander_removed_waveform_features.npy')\n",
    "fpath_eval_df_newest_ecg_waveform_data_wander_removed_pct_truncated_mean_normalized = os.path.join(home_dir,'eval_newest_ecg_per_pt_wander_removed_pct_truncated_mean_normalized_waveform_features.npy')\n",
    "#The following would be for  the binary labels you should provide\n",
    "newest_ecg_per_pt_eval_labels = os.path.join(home_dir,'eval_newest_ecg_per_pt_wander_removed_pct_truncated_mean_normalized_label.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of EKGs found:  3342\n",
      "The newest EKG is:  /Users/pae2/Box/Data/muse_XML_waveform_miniset/xmls/MUSE_XXXXXXXXXX.xml\n",
      "no PharmaUniqueECGID\n",
      "no PharmaUniqueECGID\n"
     ]
    }
   ],
   "source": [
    "def file_path(path):\n",
    "    filepath = path\n",
    "    for dirName, subdirList, fileList in os.walk(filepath):\n",
    "        for filename in fileList:\n",
    "            if \".xml\" in filename.lower():\n",
    "                ekg_file_list.append(os.path.join(dirName,filename))\n",
    "\n",
    "dir_name = npy_output_directory_path+'/'\n",
    "\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "\n",
    "def decode_ekg_muse(raw_wave):\n",
    "    \"\"\"\n",
    "    Ingest the base64 encoded waveforms and transform to numeric\n",
    "    \"\"\"\n",
    "    # covert the waveform from base64 to byte array\n",
    "    arr = base64.b64decode(bytes(raw_wave, 'utf-8'))\n",
    "\n",
    "    # unpack every 2 bytes, little endian (16 bit encoding)\n",
    "    unpack_symbols = ''.join([char*int(len(arr)/2) for char in 'h'])\n",
    "    byte_array = struct.unpack(unpack_symbols,  arr)\n",
    "    return byte_array\n",
    "\n",
    "\n",
    "def decode_ekg_muse_to_array(raw_wave, downsample = 1):\n",
    "    \"\"\"\n",
    "    Ingest the base64 encoded waveforms and transform to numeric\n",
    "\n",
    "    downsample: 0.5 takes every other value in the array. Muse samples at 500/s and the sample model requires 250/s. So take every other.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dwnsmpl = int(1//downsample)\n",
    "    except ZeroDivisionError:\n",
    "        print(\"You must downsample by more than 0\")\n",
    "    # covert the waveform from base64 to byte array\n",
    "    arr = base64.b64decode(bytes(raw_wave, 'utf-8'))\n",
    "\n",
    "    # unpack every 2 bytes, little endian (16 bit encoding)\n",
    "    unpack_symbols = ''.join([char*int(len(arr)/2) for char in 'h'])\n",
    "    byte_array = struct.unpack(unpack_symbols,  arr)\n",
    "    return np.array(byte_array)[::dwnsmpl]\n",
    "\n",
    "\n",
    "def xml_to_np_array_file(path_to_xml, path_to_output = home_dir):\n",
    "\n",
    "    with open(path_to_xml, 'rb') as fd:\n",
    "        dic = xmltodict.parse(fd.read().decode('utf8'))\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Upload the ECG as numpy array with shape=[2500,12,1] ([time, leads, 1]).\n",
    "\n",
    "    The voltage unit should be in 1 mv/unit and the sampling rate should be 250/second (total 10 second).\n",
    "\n",
    "    The leads should be ordered as follow I, II, III, aVR, aVL, aVF, V1, V2, V3, V4, V5, V6.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pt_id = dic['RestingECG']['PatientDemographics']['PatientID']\n",
    "    except:\n",
    "        print(\"no PatientID\")\n",
    "        pt_id = \"none\"\n",
    "    try:\n",
    "        PharmaUniqueECGID = dic['RestingECG']['PharmaData']['PharmaUniqueECGID']\n",
    "    except:\n",
    "        print(\"no PharmaUniqueECGID\")\n",
    "        PharmaUniqueECGID = \"none\"\n",
    "    try:\n",
    "        AcquisitionDateTime = dic['RestingECG']['TestDemographics']['AcquisitionDate'] + \"_\" + dic['RestingECG']['TestDemographics']['AcquisitionTime'].replace(\":\",\"-\")\n",
    "    except:\n",
    "        print(\"no AcquisitionDateTime\")\n",
    "        AcquisitionDateTime = \"none\"    \n",
    "        \n",
    "    #need to instantiate leads in the proper order for the model\n",
    "    lead_order = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "\n",
    "    \"\"\"\n",
    "    Each EKG will have this data structure:\n",
    "    lead_data = {\n",
    "        'I': np.array\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    lead_data =  dict.fromkeys(lead_order)\n",
    "\n",
    "    for lead in dic['RestingECG']['Waveform']:\n",
    "        for leadid in range(len(lead['LeadData'])):\n",
    "                sample_length = len(decode_ekg_muse_to_array(lead['LeadData'][leadid]['WaveFormData']))\n",
    "                #sample_length is equivalent to dic['RestingECG']['Waveform']['LeadData']['LeadSampleCountTotal']\n",
    "                if sample_length == 5000:\n",
    "                    lead_data[lead['LeadData'][leadid]['LeadID']] = decode_ekg_muse_to_array(lead['LeadData'][leadid]['WaveFormData'], downsample = 0.5)\n",
    "                elif sample_length == 2500:\n",
    "                    lead_data[lead['LeadData'][leadid]['LeadID']] = decode_ekg_muse_to_array(lead['LeadData'][leadid]['WaveFormData'], downsample = 1)\n",
    "                else:\n",
    "                    continue\n",
    "            #ensures all leads have 2500 samples and also passes over the 3 second waveform\n",
    "\n",
    "    lead_data['III'] = (np.array(lead_data[\"II\"]) - np.array(lead_data[\"I\"]))\n",
    "    lead_data['aVR'] = -(np.array(lead_data[\"I\"]) + np.array(lead_data[\"II\"]))/2\n",
    "    lead_data['aVF'] = (np.array(lead_data[\"II\"]) + np.array(lead_data[\"III\"]))/2\n",
    "    lead_data['aVL'] = (np.array(lead_data[\"I\"]) - np.array(lead_data[\"III\"]))/2\n",
    "    \n",
    "    lead_data = {k: lead_data[k] for k in lead_order}\n",
    "    # drops V3R, V4R, and V7 if it was a 15-lead ECG\n",
    "\n",
    "    # now construct and reshape the array\n",
    "    # converting the dictionary to an npy.array\n",
    "    temp = []\n",
    "    for key,value in lead_data.items():\n",
    "        temp.append(value)\n",
    "\n",
    "    #transpose to be [time, leads, ]\n",
    "    ekg_array = np.array(temp).T\n",
    "\n",
    "    #expand dims to [time, leads, 1]\n",
    "    ekg_array = np.expand_dims(ekg_array, axis=-1)\n",
    "\n",
    "    # Here is a check to make sure all the model inputs are the right shape\n",
    "#     assert ekg_array.shape == (2500, 12, 1), \"ekg_array is shape {} not (2500, 12, 1)\".format(ekg_array.shape )\n",
    "\n",
    "    filename = '{}_{}_{}.npy'.format(pt_id, AcquisitionDateTime,PharmaUniqueECGID)\n",
    "\n",
    "    path_to_output += '/'+filename\n",
    "    # print(path_to_output)\n",
    "    if not os.path.exists(dir_name + '/' + filename):\n",
    "        with open(path_to_output, 'wb') as f:\n",
    "            np.save(f, ekg_array)\n",
    "        \n",
    "\n",
    "def ekg_batch_run(ekg_list):\n",
    "    i = 0\n",
    "    x = 0\n",
    "    for file in ekg_list:\n",
    "        try:\n",
    "            xml_to_np_array_file(file, output_dir)\n",
    "            i+=1\n",
    "        except Exception as e:\n",
    "            # print(\"file failed: \", file)\n",
    "            print(file, e)\n",
    "            x+=1\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Succesfully converted {i} EKGs, failed converting {x} EKGs\")\n",
    "\n",
    "dir_name = npy_output_directory_path\n",
    "output_dir = dir_name\n",
    "ekg_file_list = []\n",
    "file_path(XML_input_directory_path)  #if you want input to be a directory\n",
    "print(\"Number of EKGs found: \", len(ekg_file_list))\n",
    "#print(type(ekg_file_list))\n",
    "ekg_file_list.sort(reverse = True)\n",
    "print(\"The newest EKG is: \", ekg_file_list[0])\n",
    "ekg_batch_run(ekg_file_list)\n",
    "\n",
    "# To reconstruct the 12 lead ecg from the array\n",
    "# test1 = np.load('waveform_output_example.npy')\n",
    "# lead_order = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "# plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "# fig, axs = plt.subplots(len(lead_data))\n",
    "# for i in range(0,12):\n",
    "#     axs[i].plot(test1[:,i])\n",
    "#     axs[i].set(ylabel=str(lead_order[i]))"
   ]
  },
  
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SELECT tdem.TestID, \n",
    "  max(pt.PatientID) as PatientID, \n",
    "  max(pt.DateofBirth_D) as DateofBirth_D, \n",
    "  max(Gender) as Gender, \n",
    "  max(Race) as Race,\n",
    "  max(AcquisitionDateTime_DT) as AcquisitionDateTime_DT,\n",
    "  max(CASE when AgeUnits = 0 and PatientAge > 0 then PatientAge ELSE null END) as PatientAge_Years,\n",
    "  max(ecg.UniqueECGID) as UniqueECGID,\n",
    "  (CAST(CASE WHEN tdem.TestID in \n",
    "        (\n",
    "        SELECT det.TestID\n",
    "        FROM BIS.MUSE_Site0013.tstDiagnosisDetails det (nolock) \n",
    "        LEFT JOIN BIS.MUSE_Site0013.cfgStatements st (nolock)on st.StatementNumber = det.StatementNumber  \n",
    "        WHERE \n",
    "        Acronym in ('VPCX', 'AVPCX', 'ASVPR', 'ASVPC', 'AVPCK', 'PCK', 'BIVPCK', 'VPR', 'AVDPR', 'WITH-DEM')\n",
    "        )\n",
    "        THEN 1 \n",
    "    ELSE 0\n",
    "    END AS int)) as Ventricular_Pacing_Present,\n",
    "  (CAST(CASE WHEN tdem.TestID in \n",
    "        (\n",
    "        SELECT det.TestID\n",
    "        FROM BIS.MUSE_Site0013.tstDiagnosisDetails det (nolock) \n",
    "        LEFT JOIN BIS.MUSE_Site0013.cfgStatements st (nolock)on st.StatementNumber = det.StatementNumber  \n",
    "        WHERE \n",
    "        Acronym in ('QCERR')\n",
    "        )\n",
    "        THEN 1 \n",
    "    ELSE 0\n",
    "    END AS int)) as Poor_Data_Quality_Present,\n",
    "    max(tst.AtrialRate) as AtrialRate,\n",
    "    max(tst.VentricularRate) as VentricularRate,\n",
    "    max(tst.P_RInterval) as P_RInterval,\n",
    "    max(tst.QRSDuration) as QRSDuration,\n",
    "\tmax(tst.QTCCalculation) as QTCCalculation,\n",
    "    max(tst.QTcFredericia) as QTcFredericia\n",
    "into #muse_pt_list\n",
    "  FROM BIS.MUSE_Site0013.tstTestDemographics tdem (nolocK) \n",
    "  LEFT JOIN BIS.MUSE_Site0013.tstDiagnosisDetails det (nolock) on tdem.TestID = det.TestID\n",
    "  LEFT JOIN BIS.MUSE_Site0013.cfgTestTypeQualifiers ctq (nolock) on ctq.TestType = tdem.TestType\n",
    "  LEFT JOIN BIS.MUSE_Site0013.cfgStatements st (nolock)on st.StatementNumber = det.StatementNumber  and st.TestType = tdem.TestType\n",
    "  LEFT JOIN BIS.MUSE_Site0013.tstPatientDemographics pt (nolock) on pt.TestID = tdem.TestID\n",
    "  LEFT JOIN BIS.MUSE_Site0013.tstPharmaTestInformation ecg (nolock) on ecg.TestID = tdem.TestID\n",
    "  LEFT JOIN BIS.MUSE_Site0013.tstRestingECGMeasurement tst (nolock) on tst.TestID = tdem.TestID\n",
    "  WHERE \n",
    "  tdem.TestID > 0\n",
    "  and PatientID <> 'NO PID'\n",
    "  GROUP BY tdem.TESTID\n",
    "  ORDER BY TESTID desc\n",
    "  '''\n",
    "\n",
    "#export these results as a csv file with the path/filename.csv that you put in query_metadata_result above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively if you do not use SQL queries to access metadata and instead are comfortable with extracting diagnostic statements from the XML file, the associated full text diagnostic statements are below. Caution, I have not used this approach in the past so there may be site differences that I am unaware of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_statements = ['*** POOR DATA QUALITY, INTERPRETATION MAY BE ADVERSELY AFFECTED',\n",
    "'ATRIAL-SENSED VENTRICULAR-PACED COMPLEXES',\n",
    "'ATRIAL-SENSED VENTRICULAR-PACED RHYTHM',\n",
    "'AV DUAL-PACED COMPLEXES',\n",
    "'AV DUAL-PACED RHYTHM',\n",
    "'AV SEQUENTIAL OR DUAL CHAMBER ELECTRONIC PACEMAKER',\n",
    "'BIVENTRICULAR PACEMAKER DETECTED',\n",
    "'ELECTRONIC VENTRICULAR PACEMAKER',\n",
    "'VENTRICULAR- PACED COMPLEXES',\n",
    "'VENTRICULAR-PACED RHYTHM',\n",
    "'WITH A DEMAND PACEMAKER']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here we generate a csv file (the path/filename you put into query_metadata_result) with the metadata from the above query, including: \n",
    "- ['TestID', 'PatientID', 'DateofBirth_D', 'Gender', 'AcquisitionDateTime_DT', 'PatientAge_Years', 'UniqueECGID', 'Ventricular_Pacing_Present', 'Poor_Data_Quality_Present', 'AtrialRate', 'VentricularRate', 'P_RInterval', 'QRSDuration', 'QTCCalculation', 'QTcFredericia']<br>\n",
    "\n",
    "## That is exported as a csv. The function below generates a filename that will match the files you created above and exported into /ekg_waveform_output"
   ]
  },
  
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pae2/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (1,2,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "eval_df = pd.read_csv(query_metadata_result)\n",
    "\n",
    "def add_filename(df): \n",
    "    df['AcquisitionDateTime_DT'] = pd.to_datetime(df['AcquisitionDateTime_DT'], errors='coerce')\n",
    "    df['FileFormatDT'] = df['AcquisitionDateTime_DT'].dt.strftime('%m-%d-%Y_%H-%M-%S')\n",
    "    FileFormatDT = ['none' if x == 'NaT' else x for x in df['FileFormatDT']] #note that the NaT here is not pd.NaT but just string \n",
    "    FileFormatDT.count('none')\n",
    "    UniqueECGID = ['none' if pd.isnull(x)==True else x for x in df['UniqueECGID']] \n",
    "    UniqueECGID.count('none')\n",
    "    df['PatientID'] = df['PatientID'].astype(str).replace('\\.0', '', regex=True)\n",
    "    PatientID = ['none' if x == 'nan' else x for x in df['PatientID']]\n",
    "    filename = []\n",
    "\n",
    "    for i in range(len(PatientID)):\n",
    "        name = str(PatientID[i]) +'_'+ str(FileFormatDT[i]) +'_'+ str(UniqueECGID[i]) + '.npy'\n",
    "        filename.append(name)\n",
    "    df['filename'] = filename\n",
    "    \n",
    "if 'filename' not in eval_df.columns:\n",
    "    add_filename(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove poor data quality and ventricularly paced ECGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = eval_df[eval_df['Ventricular_Pacing_Present']==0]\n",
    "eval_df = eval_df[eval_df['Poor_Data_Quality_Present']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add age, fill in missing values, and standard scale tabular data columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only if Age not already calculated, uncomment below\n",
    "def generate_age_multisite(df,study_date,dob):\n",
    "    df[study_date] = pd.to_datetime(df[study_date],errors='coerce')\n",
    "    df[dob] = pd.to_datetime(df[dob],errors='coerce')\n",
    "    DateofBirth_D = []\n",
    "    df['PatientAge_Years'] = (df[study_date] - df[dob]).dt.days//365\n",
    "    print(f'Number of rows where age not calculated:{np.sum(df.Age.isnull())}')\n",
    "    return df\n",
    "# eval_df = generate_age_multisite(eval_df,'AcquisitionDateTime_DT','DateofBirth_D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(df,column):\n",
    "    binary = np.array(df[column]).reshape(-1, 1)\n",
    "    enc = preprocessing.MinMaxScaler()\n",
    "    binary_scaled = enc.fit_transform(binary)\n",
    "    df[column] = binary_scaled\n",
    "    \n",
    "def binarize_standard(df,column):\n",
    "    binary = np.array(df[column]).reshape(-1, 1)\n",
    "    enc = preprocessing.StandardScaler()\n",
    "    binary_scaled = enc.fit_transform(binary)\n",
    "    df[column] = binary_scaled\n",
    "    \n",
    "enc = preprocessing.OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop any duplicate rows that occur due to poor naming \n",
    "eval_df = eval_df.drop_duplicates(subset='filename',keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Ensure waveforms exist for the metadata</ins>\n",
    "### Once you have restricted to ECGs meeting above criteria, select only the newest ECG per patient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make eval_df_available 0.0008439438800000001 seconds\n",
      "No eval_df_available found, creating now...\n",
      "eval_df length: 3192328\n",
      "eval_df available length: 2917\n",
      "eval_df ECG count: 2917\n"
     ]
    }
   ],
   "source": [
    "eval_df_available_path = Path(\"none.csv\")\n",
    "print(\"make eval_df_available\", time.process_time()/100000,'seconds')\n",
    "if not Path(eval_df_available_path).exists():\n",
    "    print(\"No eval_df_available found, creating now...\")\n",
    "    npyfilespath = npy_output_directory_path\n",
    "    os.chdir(npyfilespath)\n",
    "    npfiles = glob.glob(\"*.npy\")\n",
    "    npfiles.sort()\n",
    "    npdf = pd.DataFrame({'filename': npfiles})\n",
    "    \n",
    "    eval_df_available = eval_df.merge(npdf,left_on='filename', right_on='filename', validate='1:1', suffixes=('_df',''))\n",
    "    eval_df_available.to_csv(\"./eval_df_metadata_available.csv\",index=False)\n",
    "else:\n",
    "    print('eval_df_available found, passing in eval_df')\n",
    "    eval_df_available = eval_df\n",
    "\n",
    "filelist_eval_df = eval_df_available['filename']\n",
    "\n",
    "print('eval_df length:', len(eval_df))\n",
    "print('eval_df available length:', len(filelist_eval_df))\n",
    "print('eval_df ECG count:', len(eval_df_available))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_available['AcquisitionDateTime_DT'] = pd.to_datetime(eval_df_available['AcquisitionDateTime_DT'], errors='coerce')\n",
    "eval_newest_ecg_per_pt = eval_df_available.loc[eval_df_available.groupby(\"PatientID\")[\"AcquisitionDateTime_DT\"].idxmax()]\n",
    "assert eval_newest_ecg_per_pt['PatientID'].nunique() == len(eval_newest_ecg_per_pt), 'There is not one ECG per PatientID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_newest_ecg_per_pt_tabular = eval_newest_ecg_per_pt[TABULAR_COLUMNS_TRUNCATED]\n",
    "assert np.isnan(eval_newest_ecg_per_pt_tabular).sum(axis=0).sum() == 0, \"Error! eval_newest_ecg_per_pt_tabular has NaNs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"eval_newest_ecg_per_pt_tabular.npy\", np.array(eval_newest_ecg_per_pt_tabular).astype('float64'))\n",
    "eval_newest_ecg_per_pt.to_csv('eval_newest_ecg_per_pt_tabular_metadata.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Save Labels</ins>\n",
    "\n",
    "### These should be saved as npy arrays with binary coding under the paths you add above\n",
    "newest_ecg_per_pt_eval_labels = 'eval_newest_ecg_per_pt_wander_removed_pct_truncated_mean_normalized_label.npy'<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Build Stacked Waveform Array</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no eval_df array found, building now...\n",
      "time to generate arrays:  5.379244727000014\n",
      "eval_df_newest_ecg_waveform_data shape: (2917, 2500, 12, 1)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    eval_df_newest_ecg_waveform_data = np.load(fpath_eval_df_newest_ecg_waveform_data_prebuilt)  # Load input data. Input data should be compiled as numpy arrays with (sample numbers, time, lead, 1)\n",
    "    print('Successfully loaded pre-built eval_df_newest_ecg_waveform_data')\n",
    "except:\n",
    "    eval_df_newest_ecg_waveform_data = []\n",
    "\n",
    "time1 = timer()\n",
    "if not Path(fpath_eval_df_newest_ecg_waveform_data_prebuilt).exists():\n",
    "    print(\"no eval_df array found, building now...\")\n",
    "    for npfile in filelist_eval_df:\n",
    "        i = 0\n",
    "        try:\n",
    "            path = os.path.join(npyfilespath, npfile)\n",
    "            file = np.load(path)\n",
    "            eval_df_newest_ecg_waveform_data.append(file)\n",
    "            i += 1\n",
    "            if i % 1 == 100:\n",
    "                print(\"{i} EKGs have been written to array\")\n",
    "        except:\n",
    "            continue\n",
    "    eval_df_newest_ecg_waveform_data = np.array(eval_df_newest_ecg_waveform_data)\n",
    "    np.save(fpath_eval_df_newest_ecg_waveform_data, eval_df_newest_ecg_waveform_data)\n",
    "\n",
    "print(\"time to generate arrays: \", timer() - time1)\n",
    "print(f'eval_df_newest_ecg_waveform_data shape: {eval_df_newest_ecg_waveform_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Baseline Wander Removal</ins>\n",
    "*Note that this process is slow and only does around 240 ecgs per minute. I recommend running this notebook in screen so that this can run over the course of many hours. If you have a compute cluster and can multi-thread this it can run much faster but we haven't built that yet.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape: (2917, 2500, 12, 1) transposing now...\n",
      "1000 ECGs converted\n",
      "225.57666420936584 seconds since start\n",
      "2000 ECGs converted\n",
      "456.98662400245667 seconds since start\n",
      "(2917, 12, 2500, 1)\n",
      "completed wander removal\n"
     ]
    }
   ],
   "source": [
    "baseline_wander_removed_eval_df_newest_ecg_waveform_data = []\n",
    "\n",
    "def baseline_wander_removal(data, sampling_frequency):\n",
    "    row,__ = data.shape\n",
    "    processed_data = np.zeros(data.shape)\n",
    "    for lead in range(0,row):\n",
    "        # Baseline estimation\n",
    "        win_size = int(np.round(0.2 * sampling_frequency)) + 1\n",
    "        baseline = scipy.signal.medfilt(data[lead,:], win_size)\n",
    "        win_size = int(np.round(0.6 * sampling_frequency)) + 1\n",
    "        baseline = scipy.signal.medfilt(baseline, win_size)\n",
    "        # Removing baseline\n",
    "        filt_data = data[lead,:] - baseline\n",
    "        processed_data[lead,:] = filt_data\n",
    "    return processed_data\n",
    "\n",
    "def baseline_wander_removal_batch(dataset,outputlist):\n",
    "    a=0\n",
    "    time1 = time.time()\n",
    "\n",
    "    if dataset.shape[1:] == (2500,12,1):\n",
    "        print('dataset shape:',dataset.shape,'transposing now...')\n",
    "        dataset = np.transpose(dataset, axes=[0,2,1,3])\n",
    "    elif dataset.shape[1:] == (1,2500,12):\n",
    "        print('dataset shape:', dataset.shape, 'transposing now...')\n",
    "        dataset = np.transpose(dataset, axes=[0,3,2,1])\n",
    "    elif dataset.shape[1:] == (12,2500,1):\n",
    "        pass\n",
    "    for ecg in dataset:\n",
    "        assert ecg.shape == (12,2500,1), \"ecg is not 12,2500\"\n",
    "        a+=1\n",
    "        processed_data = baseline_wander_removal(ecg.squeeze(),250)\n",
    "        outputlist.append(processed_data)\n",
    "        if a%1000==0:\n",
    "            print(a,'ECGs converted')\n",
    "            print(time.time()-time1,\"seconds since start\")\n",
    "\n",
    "    outputlist = np.expand_dims(np.array(outputlist),axis=3)\n",
    "    print(outputlist.shape)\n",
    "\n",
    "baseline_wander_removal_batch(eval_df_newest_ecg_waveform_data,baseline_wander_removed_eval_df_newest_ecg_waveform_data)\n",
    "\n",
    "np.save(fpath_eval_df_newest_ecg_waveform_data_wander_removed,baseline_wander_removed_eval_df_newest_ecg_waveform_data)\n",
    "print('completed wander removal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Per-Lead Truncation and Mean Normalization</ins>\n",
    "*We've now dealt with a major form of bias by removing baseline wander. But there are still significant outliers that are physiologically unimportant. So we truncate to the 0.1st and 99.9th percentile of the data internally, then do per-lead normalization using the mean and STD of the Columbia train dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_data_percentile(target_array, lowerbound, upperbound):\n",
    "    datapctlimit = target_array\n",
    "    assert (len(upperbound) == datapctlimit.shape[1]), \"shape of upperbound does not match array.shape[1]\"\n",
    "    # Truncate data to 0.1th and 99.9th percentile\n",
    "    for i in range(len(upperbound)):\n",
    "        print(\"pre-truncation min:\",datapctlimit[:, i, :, :].min())\n",
    "        print(\"pre-truncation max:\", datapctlimit[:, i, :, :].max())\n",
    "        datapctlimit[:, i, :, :] = np.where(datapctlimit[:, i, :, :] > upperbound[i], upperbound[i], datapctlimit[:, i, :, :])\n",
    "        datapctlimit[:, i, :, :] = np.where(datapctlimit[:, i, :, :] < lowerbound[i], lowerbound[i], datapctlimit[:, i, :, :])\n",
    "    for i in range(len(upperbound)):\n",
    "        print(\"post-truncation min:\",datapctlimit[:, i, :, :].min())\n",
    "        print(\"post-truncation max:\", datapctlimit[:, i, :, :].max())\n",
    "    # datanorm2 = cv2.normalize(datapctlimit, datapctlimit, -1, 1, cv2.NORM_MINMAX)\n",
    "    return datapctlimit\n",
    "\n",
    "def per_lead_truncation_normalization_prospective(dataset_data,save_path):\n",
    "    if dataset_data.shape[1:] == (2500,12,1):\n",
    "        print('dataset_data shape:',dataset_data.shape,'transposing now...')\n",
    "        dataset_data = np.transpose(dataset_data, axes=[0,2,1,3])\n",
    "    elif dataset_data.shape[1:] == (1,2500,12):\n",
    "        print('dataset_data shape:', dataset_data.shape, 'transposing now...')\n",
    "        dataset_data = np.transpose(dataset_data, axes=[0,3,2,1])\n",
    "    elif dataset_data.shape[1:] == (12,2500,1):\n",
    "        pass\n",
    "    assert dataset_data.shape[1:] == (12, 2500, 1), \"dataset is not X,12,2500,1\"\n",
    "\n",
    "    # Compute mean and standard deviation for normalization\n",
    "    mean_tr, std_tr = np.mean(dataset_data, axis=(0, 2, 3)), np.std(dataset_data, axis=(0, 2, 3))\n",
    "    print('Mean and STD before truncation')\n",
    "    print(mean_tr)\n",
    "    print(std_tr)\n",
    "\n",
    "    # med_tr, mad_tr = np.median(dataset_data, axis=(0, 2, 3)), stats.median_absolute_deviation(dataset_data, axis=(0, 2, 3))\n",
    "    # print(\"Median and MAD before truncation\", mean_tr, std_tr)\n",
    "\n",
    "    lowerbound = []\n",
    "    upperbound = []\n",
    "    for i in range(0,12):\n",
    "        dataset = dataset_data\n",
    "        lowerbound.append(np.percentile(dataset[:,i,:,:],0.1))\n",
    "        upperbound.append(np.percentile(dataset[:,i,:,:],99.9))\n",
    "\n",
    "    print(\"dataset 0.1st percentile lowerbound:\",lowerbound)\n",
    "    print(\"dataset 99.9th percentile upperbound:\",upperbound)\n",
    "\n",
    "\n",
    "    # NOTE: NEED TO put Muse_Union_Syngo_data before dataset_data \n",
    "    dataset_data = truncate_data_percentile(dataset_data, lowerbound, upperbound)\n",
    "    print(\"dataset data truncated based on percentiles\")\n",
    "\n",
    "    mean_tr, std_tr = np.mean(dataset_data, axis=(0, 2, 3)), np.std(dataset_data, axis=(0, 2, 3))\n",
    "    print('Mean and STD before normalization', mean_tr, std_tr)\n",
    "\n",
    "    # For validation and prospective trial must use train mean and standard deviation\n",
    "    # These values comes from /labs/amyloid/CardiacAmyloid/ValveNetExtraData/220k_multivalve_train_pace_removed_poor_quality_removed_wander_removed_waveform_features.npy\n",
    "    mean_tr = [5.3913547,4.08127771,-1.18338815,-4.72457889,0.59169407,-0.59169407,-4.25676191,-1.44444217,-0.34211766,3.35517074,5.18700818,5.53535442]\n",
    "    std_tr = [31.1319959,29.16014537,30.41666735,26.0615433,15.20833367,15.20833367,38.35645889,53.62205647,58.92661479,49.45070532,43.92990916,38.29998916]\n",
    "\n",
    "    # Per-lead normalization (there are 12 leads)\n",
    "    assert (len(mean_tr) == dataset_data.shape[1])\n",
    "    for i in range(len(mean_tr)):\n",
    "        tic = time.perf_counter()\n",
    "        print(\"pre-normalizing min:\",dataset_data[:, i, :, :].min())\n",
    "        print(\"pre-normalizing max:\", dataset_data[:, i, :, :].max())\n",
    "        dataset_data[:, i, :, :] = (dataset_data[:, i, :, :] - mean_tr[i]) / std_tr[i]\n",
    "        print(\"post-normalizing min:\",dataset_data[:, i, :, :].min())\n",
    "        print(\"post-normalizing max:\", dataset_data[:, i, :, :].max())\n",
    "        toc = time.perf_counter()\n",
    "        print(f'Processed iter {i} in {toc - tic:0.2f} seconds')\n",
    "\n",
    "    mean_norm, std_norm = np.mean(dataset_data, axis=(0, 2, 3)), np.std(dataset_data, axis=(0, 2, 3))\n",
    "    print('Mean and STD after normalization', mean_norm, std_norm)\n",
    "    # med_norm, mad_norm = np.median(dataset_data, axis=(0, 2, 3)), stats.median_absolute_deviation(dataset_data, axis=(0, 2, 3))\n",
    "    # print(\"Median and MAD after normalization\", med_norm, mad_norm)\n",
    "\n",
    "    # Transpose data for model\n",
    "    X_dataset = np.transpose(dataset_data, axes=[0, 3, 2, 1])\n",
    "    print('X_dataset shape for model:', X_dataset.shape)\n",
    "    assert X_dataset.shape[1:] == (1, 2500, 12), \"dataset is not X,1,2500,12\"\n",
    "\n",
    "    # Write out feature arrays\n",
    "    dataset_features_path = save_path #'/path/file.npy'\n",
    "    np.save(dataset_features_path, X_dataset)\n",
    "    print('Saved feature arrays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean and STD before truncation\n",
      "[ 5.97941282  5.87813493  0.09237943 -5.93905903 -0.04618972  0.04618972\n",
      " -4.66775358 -0.95858238  1.09394858  5.22302448  6.79488708  6.6911616 ]\n",
      "[32.31957555 33.95522112 31.36092133 29.19812889 15.68046067 15.68046067\n",
      " 40.18049874 53.62923659 61.15692121 55.13048428 49.13032946 41.64655158]\n",
      "dataset 0.1st percentile lowerbound: [-151.0, -167.0, -282.0, -257.5, -132.5, -141.0, -360.0, -489.0, -563.0, -424.0, -285.0, -202.0]\n",
      "dataset 99.9th percentile upperbound: [292.0, 324.0, 265.0, 115.0, 141.0, 132.5, 241.0, 326.0, 413.0, 464.0, 463.0, 403.0]\n",
      "pre-truncation min: -648.0\n",
      "pre-truncation max: 732.0\n",
      "pre-truncation min: -504.0\n",
      "pre-truncation max: 754.0\n",
      "pre-truncation min: -940.0\n",
      "pre-truncation max: 916.0\n",
      "pre-truncation min: -728.0\n",
      "pre-truncation max: 564.0\n",
      "pre-truncation min: -458.0\n",
      "pre-truncation max: 470.0\n",
      "pre-truncation min: -470.0\n",
      "pre-truncation max: 458.0\n",
      "pre-truncation min: -910.0\n",
      "pre-truncation max: 1424.0\n",
      "pre-truncation min: -1094.0\n",
      "pre-truncation max: 979.0\n",
      "pre-truncation min: -1182.0\n",
      "pre-truncation max: 1600.0\n",
      "pre-truncation min: -1154.0\n",
      "pre-truncation max: 1686.0\n",
      "pre-truncation min: -1390.0\n",
      "pre-truncation max: 1285.0\n",
      "pre-truncation min: -994.0\n",
      "pre-truncation max: 1199.0\n",
      "post-truncation min: -151.0\n",
      "post-truncation max: 292.0\n",
      "post-truncation min: -167.0\n",
      "post-truncation max: 324.0\n",
      "post-truncation min: -282.0\n",
      "post-truncation max: 265.0\n",
      "post-truncation min: -257.5\n",
      "post-truncation max: 115.0\n",
      "post-truncation min: -132.5\n",
      "post-truncation max: 141.0\n",
      "post-truncation min: -141.0\n",
      "post-truncation max: 132.5\n",
      "post-truncation min: -360.0\n",
      "post-truncation max: 241.0\n",
      "post-truncation min: -489.0\n",
      "post-truncation max: 326.0\n",
      "post-truncation min: -563.0\n",
      "post-truncation max: 413.0\n",
      "post-truncation min: -424.0\n",
      "post-truncation max: 464.0\n",
      "post-truncation min: -285.0\n",
      "post-truncation max: 463.0\n",
      "post-truncation min: -202.0\n",
      "post-truncation max: 403.0\n",
      "dataset data truncated based on percentiles\n",
      "Mean and STD before normalization [ 5.99222228  5.86555201  0.07484374 -5.94379637 -0.03742187  0.03742187\n",
      " -4.71325238 -0.95084429  1.04702503  5.20494206  6.79947919  6.69732588] [31.26733173 32.89158582 29.16080708 28.44871254 14.58040354 14.58040354\n",
      " 37.77662239 51.23742576 57.84758489 51.65854033 46.0751591  39.11361151]\n",
      "pre-normalizing min: -151.0\n",
      "pre-normalizing max: 292.0\n",
      "post-normalizing min: -5.0234927179853575\n",
      "post-normalizing max: 9.206240622047622\n",
      "Processed iter 0 in 0.06 seconds\n",
      "pre-normalizing min: -167.0\n",
      "pre-normalizing max: 324.0\n",
      "post-normalizing min: -5.86695558404207\n",
      "post-normalizing max: 10.971094904730238\n",
      "Processed iter 1 in 0.06 seconds\n",
      "pre-normalizing min: -282.0\n",
      "pre-normalizing max: 265.0\n",
      "post-normalizing min: -9.232326757520331\n",
      "post-normalizing max: 8.751234482301033\n",
      "Processed iter 2 in 0.06 seconds\n",
      "pre-normalizing min: -257.5\n",
      "pre-normalizing max: 115.0\n",
      "post-normalizing min: -9.69917315334123\n",
      "post-normalizing max: 4.593917463437402\n",
      "Processed iter 3 in 0.07 seconds\n",
      "pre-normalizing min: -132.5\n",
      "pre-normalizing max: 141.0\n",
      "post-normalizing min: -8.751234484849384\n",
      "post-normalizing max: 9.232326760884384\n",
      "Processed iter 4 in 0.06 seconds\n",
      "pre-normalizing min: -141.0\n",
      "pre-normalizing max: 132.5\n",
      "post-normalizing min: -9.232326760884384\n",
      "post-normalizing max: 8.751234484849384\n",
      "Processed iter 5 in 0.06 seconds\n",
      "pre-normalizing min: -360.0\n",
      "pre-normalizing max: 241.0\n",
      "post-normalizing min: -9.274663208879968\n",
      "post-normalizing max: 6.394145054249037\n",
      "Processed iter 6 in 0.06 seconds\n",
      "pre-normalizing min: -489.0\n",
      "pre-normalizing max: 326.0\n",
      "post-normalizing min: -9.092444227736275\n",
      "post-normalizing max: 6.106525257068344\n",
      "Processed iter 7 in 0.06 seconds\n",
      "pre-normalizing min: -563.0\n",
      "pre-normalizing max: 413.0\n",
      "post-normalizing min: -9.548450803515095\n",
      "post-normalizing max: 7.014523388676745\n",
      "Processed iter 8 in 0.06 seconds\n",
      "pre-normalizing min: -424.0\n",
      "pre-normalizing max: 464.0\n",
      "post-normalizing min: -8.642043990566888\n",
      "post-normalizing max: 9.315232740951329\n",
      "Processed iter 9 in 0.06 seconds\n",
      "pre-normalizing min: -285.0\n",
      "pre-normalizing max: 463.0\n",
      "post-normalizing min: -6.605681954021141\n",
      "post-normalizing max: 10.421441805230447\n",
      "Processed iter 10 in 0.06 seconds\n",
      "pre-normalizing min: -202.0\n",
      "pre-normalizing max: 403.0\n",
      "post-normalizing min: -5.418679194738446\n",
      "post-normalizing max: 10.377669923601617\n",
      "Processed iter 11 in 0.06 seconds\n",
      "Mean and STD after normalization [ 0.01930064  0.0611888   0.04136653 -0.04678224 -0.04136653  0.04136653\n",
      " -0.01190127  0.00920513  0.02357411  0.03740637  0.03670554  0.03033869] [1.00434716 1.12796371 0.95871144 1.09159739 0.95871144 0.95871144\n",
      " 0.98488295 0.95552892 0.98168858 1.04464719 1.04883347 1.02124341]\n",
      "X_dataset shape for model: (2917, 1, 2500, 12)\n",
      "Saved feature arrays\n"
     ]
    }
   ],
   "source": [
    "eval_df_newest_ecg_waveform_data_wander_removed = np.load(fpath_eval_df_newest_ecg_waveform_data_wander_removed)\n",
    "if len(eval_df_newest_ecg_waveform_data_wander_removed.shape) == 3:\n",
    "    eval_df_newest_ecg_waveform_data_wander_removed = np.expand_dims(eval_df_newest_ecg_waveform_data_wander_removed,axis=3)\n",
    "\n",
    "#run the normalization function and save the output waveform array which whill then be used for evaluation\n",
    "per_lead_truncation_normalization_prospective(eval_df_newest_ecg_waveform_data_wander_removed, fpath_eval_df_newest_ecg_waveform_data_wander_removed_pct_truncated_mean_normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
