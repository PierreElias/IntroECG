{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import cv2\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_auc_score, \\\n",
    "    average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from skimage import io\n",
    "import copy\n",
    "from torch import optim, cuda\n",
    "import pandas as pd\n",
    "import glob\n",
    "from collections import Counter\n",
    "# Useful for examining network\n",
    "from functools import reduce\n",
    "from operator import __add__\n",
    "# from torchsummary import summary\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Useful for examining network\n",
    "from functools import reduce\n",
    "from operator import __add__\n",
    "from torchsummary import summary\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Image manipulations\n",
    "from PIL import Image\n",
    "\n",
    "# Timing utility\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('/data/proj/cardiac-amyloid/ValveNet/data/57k_train_df_pace_removed_poor_quality_removed_any_patient_negative_as_tabular_metadata.csv')\n",
    "# eval_df = pd.read_csv('/data/proj/cardiac-amyloid/ValveNet/data/29k_eval_df_pace_removed_poor_quality_removed_any_patient_negative_as_tabular_metadata.csv')\n",
    "train_df = pd.read_csv('/data/proj/cardiac-amyloid/ValveNet/data/57k_train_df_pace_removed_2x_poor_quality_removed_any_patient_negative_as_tabular_metadata.csv')\n",
    "eval_df = pd.read_csv('/data/proj/cardiac-amyloid/ValveNet/data/29k_eval_df_pace_removed_2x_poor_quality_removed_any_patient_negative_as_tabular_metadata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_binary(df, label, sampling_strategy=1):\n",
    "    y = df[label]\n",
    "    X = df\n",
    "    binary_mask = np.bitwise_or(y == 0, y == 1)\n",
    "    binary_y = y[binary_mask]\n",
    "    binary_X = X[binary_mask]\n",
    "    rus = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "    X_res, y_res = rus.fit_resample(binary_X, binary_y)\n",
    "    return X_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make train_df_available 0.14609136083105 seconds\n",
      "train_df_available found, passing in train_df\n"
     ]
    }
   ],
   "source": [
    "# train_available_path = Path(\"/data/proj/cardiac-amyloid/ValveNet/data/50k_train_df_pace_removed_any_patient_negative_available_for_GCP_server.csv\")\n",
    "train_available_path = Path(\"/data/proj/cardiac-amyloid/ValveNet/data/57k_train_df_pace_removed_2x_poor_quality_removed_any_patient_negative_as_tabular_metadata.csv\")\n",
    "#TODO this only works if this file path doesnt already exist\n",
    "print(\"make train_df_available\", time.process_time()/100000,'seconds')\n",
    "if not Path(train_available_path).exists():\n",
    "    print(\"No train_df_available found, creating now...\")\n",
    "    npyfilespath = \"/data/proj/cardiac-amyloid/ekg_waveforms_output/\"\n",
    "    os.chdir(npyfilespath)\n",
    "    npfiles = glob.glob(\"*.npy\")\n",
    "    npfiles.sort()\n",
    "    npdf = pd.DataFrame({'filename': npfiles})\n",
    "    train_df_available = train_df.merge(npdf,left_on='filename', right_on='filename', validate='1:1')\n",
    "    eval_df_available = eval_df.merge(npdf,left_on='filename', right_on='filename', validate='1:1')\n",
    "\n",
    "    # print(\"balance datasets\", time.process_time())\n",
    "    label = \"stenosis_label_binary\"\n",
    "    # train_df_available = balance_binary(train_df_available, label)\n",
    "    # eval_df_available = balance_binary(eval_df_available,label)\n",
    "    train_df_available = train_df_available\n",
    "    eval_df_available = eval_df_available\n",
    "    train_df_available.to_csv(\"/data/proj/cardiac-amyloid/ValveNet/data/57k_train_df_pace_removed_2x_poor_quality_removed_any_patient_negative_as_tabular_metadata.csv\")\n",
    "    eval_df_available.to_csv(\"/data/proj/cardiac-amyloid/ValveNet/data/29k_eval_df_pace_removed_2x_poor_quality_removed_any_patient_negative_as_tabular_metadata.csv\")\n",
    "else:\n",
    "    print('train_df_available found, passing in train_df')\n",
    "    train_df_available = train_df\n",
    "    eval_df_available = eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist_train = train_df_available['filename']\n",
    "filelist_eval = eval_df_available['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train DF length: 55593 \n",
      " eval DF length: 27984\n",
      "train DF available length: 55593 \n",
      " eval DF available length: 27984\n",
      "train ECG count: 55593 \n",
      " eval ECG count: 27984\n"
     ]
    }
   ],
   "source": [
    "print('train DF length:', len(train_df), '\\n eval DF length:', len(eval_df))\n",
    "print('train DF available length:', len(filelist_train), '\\n eval DF available length:', len(filelist_eval))\n",
    "print('train ECG count:', len(train_df_available), '\\n eval ECG count:', len(eval_df_available))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "npyfilespath = \"/data/proj/cardiac-amyloid/ekg_waveforms_output/\"\n",
    "fpath_train = \"/data/proj/cardiac-amyloid/ValveNet/data/57k_train_df_pace_removed_2x_poor_quality_removed_any_patient_negative_as_tabular_metadata.npy\"\n",
    "fpath_eval = \"/data/proj/cardiac-amyloid/ValveNet/data/29k_eval_df_pace_removed_2x_poor_quality_removed_any_patient_negative_as_tabular_metadata.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded pre-built trainData and evalData\n",
      "time to generate arrays:  0.00041944533586502075\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainData = np.load(fpath_train)  # Load input data. Input data should be compiled as numpy arrays with (sample numbers, time, lead, 1)\n",
    "    evalData = np.load(fpath_eval)\n",
    "    print('Successfully loaded pre-built trainData and evalData')\n",
    "except:\n",
    "    trainData = []\n",
    "    evalData = []\n",
    "\n",
    "time1 = timer()\n",
    "if not Path(fpath_train).exists():  # or (len(trainData)!=len(train_df_available)) or (len(evalData)!=len(eval_df_available)):\n",
    "    print(\"no train array found, building now...\")\n",
    "    for npfile in filelist_train:\n",
    "        i = 0\n",
    "        try:\n",
    "            path = os.path.join(npyfilespath + npfile)\n",
    "            file = np.load(path)\n",
    "            trainData.append(file)\n",
    "            i += 1\n",
    "            if i % 1 == 100:\n",
    "                print(\"{i} EKGs have been written to array\")\n",
    "        except:\n",
    "            continue\n",
    "    trainData = np.array(trainData)\n",
    "    np.save(fpath_train, trainData)\n",
    "\n",
    "if not Path(fpath_eval).exists():  # or (len(trainData)!=len(train_df_available)) or (len(evalData)!=len(eval_df_available)):\n",
    "    print(\"no eval array found, building now...\")\n",
    "    for npfile in filelist_eval:\n",
    "        i = 0\n",
    "        try:\n",
    "            path = os.path.join(npyfilespath + npfile)\n",
    "            file = np.load(path)\n",
    "            evalData.append(file)\n",
    "            i += 1\n",
    "            if i % 1 == 100:\n",
    "                print(\"{i} EKGs have been written to array\")\n",
    "        except:\n",
    "            continue\n",
    "    evalData = np.array(evalData)\n",
    "    np.save(fpath_eval, evalData)\n",
    "print(\"time to generate arrays: \", timer() - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55593, 2500, 12, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unprocessed_train shape for model: (55593, 1, 2500, 12)\n",
      "unprocessed_eval shape for model: (27984, 1, 2500, 12)\n"
     ]
    }
   ],
   "source": [
    "unprocessed_train = np.transpose(trainData, axes=[0, 3, 1, 2])\n",
    "print('unprocessed_train shape for model:', unprocessed_train.shape)\n",
    "unprocessed_eval = np.transpose(evalData, axes=[0, 3, 1, 2])\n",
    "print('unprocessed_eval shape for model:', unprocessed_eval.shape)\n",
    "assert unprocessed_train.shape[1:] == (1, 2500, 12), \"train is not X,1,2500,12\"\n",
    "assert unprocessed_eval.shape[1:] == (1, 2500, 12), \"eval is not X,1,2500,12\"\n",
    "\n",
    "# Write out feature arrays unprocessed if desired\n",
    "# np.save('/data/proj/cardiac-amyloid/ValveNet/data/57k_train_df_pace_removed_2x_poor_quality_removed_any_patient_negative_as_features_unprocessed.npy', unprocessed_train)\n",
    "# np.save('/data/proj/cardiac-amyloid/ValveNet/data/29k_eval_df_pace_removed_2x_poor_quality_removed_any_patient_negative_as_features_unprocessed.npy', unprocessed_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data norm_prep shape: (55593, 12, 2500, 1)\n",
      "eval_data norm_prep shape: (27984, 12, 2500, 1)\n",
      "Mean and STD before truncation [-0.57058358 -0.52162918  0.04895439  0.54610638 -0.0244772   0.0244772\n",
      " -0.65568828 -0.68845719 -0.79558483 -0.67047087 -0.66145356 -0.67744864] [36.69501863 34.69142697 38.8391823  29.96479983 19.41959115 19.41959115\n",
      " 47.15307279 66.56324286 72.08039239 59.55403582 52.06029694 46.56159721]\n",
      "Median and MAD before truncation [-5.  -4.   2.   4.5 -1.   1.   4.   2.   1.  -2.  -4.  -5. ] [11.8608 13.3434 11.8608 11.1195  5.9304  5.9304 13.3434 17.7912 19.2738\n",
      " 17.7912 16.3086 14.826 ]\n",
      "pre-truncation min: -2019.0\n",
      "pre-truncation max: 1294.0\n",
      "pre-truncation min: -7428.0\n",
      "pre-truncation max: 4314.0\n",
      "pre-truncation min: -7352.0\n",
      "pre-truncation max: 4214.0\n",
      "pre-truncation min: -2207.0\n",
      "pre-truncation max: 3752.0\n",
      "pre-truncation min: -2107.0\n",
      "pre-truncation max: 3676.0\n",
      "pre-truncation min: -3676.0\n",
      "pre-truncation max: 2107.0\n",
      "pre-truncation min: -32188.0\n",
      "pre-truncation max: 31514.0\n",
      "pre-truncation min: -5186.0\n",
      "pre-truncation max: 2603.0\n",
      "pre-truncation min: -23889.0\n",
      "pre-truncation max: 2718.0\n",
      "pre-truncation min: -2067.0\n",
      "pre-truncation max: 2398.0\n",
      "pre-truncation min: -1947.0\n",
      "pre-truncation max: 2306.0\n",
      "pre-truncation min: -1031.0\n",
      "pre-truncation max: 2388.0\n",
      "post-truncation min: -64.304\n",
      "post-truncation max: 54.303999999999995\n",
      "post-truncation min: -70.717\n",
      "post-truncation max: 62.717\n",
      "post-truncation min: -57.303999999999995\n",
      "post-truncation max: 61.303999999999995\n",
      "post-truncation min: -51.0975\n",
      "post-truncation max: 60.0975\n",
      "post-truncation min: -30.651999999999997\n",
      "post-truncation max: 28.651999999999997\n",
      "post-truncation min: -28.651999999999997\n",
      "post-truncation max: 30.651999999999997\n",
      "post-truncation min: -62.717\n",
      "post-truncation max: 70.717\n",
      "post-truncation min: -86.956\n",
      "post-truncation max: 90.956\n",
      "post-truncation min: -95.36899999999999\n",
      "post-truncation max: 97.36899999999999\n",
      "post-truncation min: -90.956\n",
      "post-truncation max: 86.956\n",
      "post-truncation min: -85.54299999999999\n",
      "post-truncation max: 77.54299999999999\n",
      "post-truncation min: -79.13\n",
      "post-truncation max: 69.13\n",
      "eval data truncated based on median\n",
      "pre-truncation min: -7761.0\n",
      "pre-truncation max: 6602.0\n",
      "pre-truncation min: -5859.0\n",
      "pre-truncation max: 8170.0\n",
      "pre-truncation min: -6830.0\n",
      "pre-truncation max: 15931.0\n",
      "pre-truncation min: -6565.5\n",
      "pre-truncation max: 5702.5\n",
      "pre-truncation min: -7965.5\n",
      "pre-truncation max: 3415.0\n",
      "pre-truncation min: -3415.0\n",
      "pre-truncation max: 7965.5\n",
      "pre-truncation min: -6762.0\n",
      "pre-truncation max: 6603.0\n",
      "pre-truncation min: -8030.0\n",
      "pre-truncation max: 24268.0\n",
      "pre-truncation min: -11850.0\n",
      "pre-truncation max: 13202.0\n",
      "pre-truncation min: -11698.0\n",
      "pre-truncation max: 8896.0\n",
      "pre-truncation min: -9932.0\n",
      "pre-truncation max: 12056.0\n",
      "pre-truncation min: -10974.0\n",
      "pre-truncation max: 13553.0\n",
      "post-truncation min: -64.304\n",
      "post-truncation max: 54.303999999999995\n",
      "post-truncation min: -70.717\n",
      "post-truncation max: 62.717\n",
      "post-truncation min: -57.303999999999995\n",
      "post-truncation max: 61.303999999999995\n",
      "post-truncation min: -51.0975\n",
      "post-truncation max: 60.0975\n",
      "post-truncation min: -30.651999999999997\n",
      "post-truncation max: 28.651999999999997\n",
      "post-truncation min: -28.651999999999997\n",
      "post-truncation max: 30.651999999999997\n",
      "post-truncation min: -62.717\n",
      "post-truncation max: 70.717\n",
      "post-truncation min: -86.956\n",
      "post-truncation max: 90.956\n",
      "post-truncation min: -95.36899999999999\n",
      "post-truncation max: 97.36899999999999\n",
      "post-truncation min: -90.956\n",
      "post-truncation max: 86.956\n",
      "post-truncation min: -85.54299999999999\n",
      "post-truncation max: 77.54299999999999\n",
      "post-truncation min: -79.13\n",
      "post-truncation max: 69.13\n",
      "train data truncated based on median\n",
      "Mean and STD before normalization [-3.45233916 -1.71004031  1.77312618  2.52600273 -0.88656309  0.88656309\n",
      "  2.62832039  3.20430291  3.19902776 -0.26345014 -2.62338416 -3.41372448] [20.64529118 22.41031376 20.92426257 18.66711502 10.46213128 10.46213128\n",
      " 24.44478172 33.43660663 37.03091841 32.74266153 28.6589763  25.43818501]\n",
      "Median and MAD before normalization [-5.  -4.   2.   4.5 -1.   1.   4.   2.   1.  -2.  -4.  -5. ] [11.8608 13.3434 11.8608 11.1195  5.9304  5.9304 13.3434 17.7912 19.2738\n",
      " 17.7912 16.3086 14.826 ]\n",
      "pre-normalizing min: -64.304\n",
      "pre-normalizing max: 54.303999999999995\n",
      "post-normalizing min: -5.000000000000001\n",
      "post-normalizing max: 5.0\n",
      "Processed iter 0 in 12.62 seconds\n",
      "pre-normalizing min: -70.717\n",
      "pre-normalizing max: 62.717\n",
      "post-normalizing min: -5.0\n",
      "post-normalizing max: 5.0\n",
      "Processed iter 1 in 7.85 seconds\n",
      "pre-normalizing min: -57.303999999999995\n",
      "pre-normalizing max: 61.303999999999995\n",
      "post-normalizing min: -5.0\n",
      "post-normalizing max: 5.0\n",
      "Processed iter 2 in 7.94 seconds\n",
      "pre-normalizing min: -51.0975\n",
      "pre-normalizing max: 60.0975\n",
      "post-normalizing min: -5.0\n",
      "post-normalizing max: 5.0\n",
      "Processed iter 3 in 7.90 seconds\n",
      "pre-normalizing min: -30.651999999999997\n",
      "pre-normalizing max: 28.651999999999997\n",
      "post-normalizing min: -5.0\n",
      "post-normalizing max: 5.0\n",
      "Processed iter 4 in 7.90 seconds\n",
      "pre-normalizing min: -28.651999999999997\n",
      "pre-normalizing max: 30.651999999999997\n",
      "post-normalizing min: -5.0\n",
      "post-normalizing max: 5.0\n",
      "Processed iter 5 in 7.87 seconds\n",
      "pre-normalizing min: -62.717\n",
      "pre-normalizing max: 70.717\n",
      "post-normalizing min: -5.0\n",
      "post-normalizing max: 5.0\n",
      "Processed iter 6 in 7.91 seconds\n",
      "pre-normalizing min: -86.956\n",
      "pre-normalizing max: 90.956\n",
      "post-normalizing min: -5.0\n",
      "post-normalizing max: 5.0\n",
      "Processed iter 7 in 7.90 seconds\n",
      "pre-normalizing min: -95.36899999999999\n",
      "pre-normalizing max: 97.36899999999999\n",
      "post-normalizing min: -5.0\n",
      "post-normalizing max: 5.0\n",
      "Processed iter 8 in 7.89 seconds\n",
      "pre-normalizing min: -90.956\n",
      "pre-normalizing max: 86.956\n",
      "post-normalizing min: -5.0\n",
      "post-normalizing max: 5.0\n",
      "Processed iter 9 in 7.90 seconds\n",
      "pre-normalizing min: -85.54299999999999\n",
      "pre-normalizing max: 77.54299999999999\n",
      "post-normalizing min: -5.0\n",
      "post-normalizing max: 5.0\n",
      "Processed iter 10 in 7.90 seconds\n",
      "pre-normalizing min: -79.13\n",
      "pre-normalizing max: 69.13\n",
      "post-normalizing min: -5.0\n",
      "post-normalizing max: 5.0\n",
      "Processed iter 11 in 7.89 seconds\n",
      "Mean and STD after normalization [ 0.13048537  0.17161741 -0.01912804 -0.17752572  0.01912804 -0.01912804\n",
      " -0.10279836  0.06769093  0.11409415  0.09760724  0.08441042  0.10699282] [1.74063226 1.67950551 1.76415272 1.67877288 1.76415272 1.76415272\n",
      " 1.83197549 1.87939018 1.92130864 1.84038522 1.75729224 1.71578207]\n",
      "Median and MAD after normalization [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "X_train shape for model: (55593, 1, 2500, 12)\n",
      "X_eval shape for model: (27984, 1, 2500, 12)\n"
     ]
    }
   ],
   "source": [
    "# # Transpose for normalization\n",
    "'''02/06/21 update, truncate_data_median now does per-lead truncation.\n",
    "The median and MAD are recaluclated after truncation is complete \n",
    "and eval is normalized before train, neither should make significant differences but are more correct'''\n",
    "\n",
    "train_data = np.transpose(trainData, axes=[0, 2, 1, 3])\n",
    "print('train_data norm_prep shape:', train_data.shape)\n",
    "eval_data = np.transpose(evalData, axes=[0, 2, 1, 3])\n",
    "print('eval_data norm_prep shape:', eval_data.shape)\n",
    "assert train_data.shape[1:] == (12, 2500, 1), \"train is not X,12,2500,1\"\n",
    "assert eval_data.shape[1:] == (12, 2500, 1), \"eval is not X,12,2500,1\"\n",
    "\n",
    "sigma=5\n",
    "\n",
    "def truncate_data_median(target_array, median, mad):\n",
    "    datamadlimit = target_array\n",
    "    assert (len(med_tr) == datamadlimit.shape[1])\n",
    "    # Truncate data to sigma*mad to limit outlier effect and use robust statistical approach over mean\n",
    "    for i in range(len(median)):\n",
    "        print(\"pre-truncation min:\",datamadlimit[:, i, :, :].min())\n",
    "        print(\"pre-truncation max:\", datamadlimit[:, i, :, :].max())\n",
    "        datamadlimit[:, i, :, :] = np.where(datamadlimit[:, i, :, :] > median[i] + (sigma * mad[i]), median[i] + (sigma * mad[i]), datamadlimit[:, i, :, :])\n",
    "        datamadlimit[:, i, :, :] = np.where(datamadlimit[:, i, :, :] < median[i] - (sigma * mad[i]), median[i] - (sigma * mad[i]), datamadlimit[:, i, :, :])\n",
    "    for i in range(len(median)):\n",
    "        print(\"post-truncation min:\",datamadlimit[:, i, :, :].min())\n",
    "        print(\"post-truncation max:\", datamadlimit[:, i, :, :].max())\n",
    "    # datanorm2 = cv2.normalize(datamadlimit, datamadlimit, -1, 1, cv2.NORM_MINMAX)\n",
    "    return datamadlimit\n",
    "\n",
    "\n",
    "# Compute mean and standard deviation for normalization\n",
    "# median = np.median(train_data)\n",
    "# mad = stats.median_absolute_deviation(train_data)\n",
    "mean_norm, std_norm = np.mean(train_data, axis=(0, 2, 3)), np.std(train_data, axis=(0, 2, 3))\n",
    "print('Mean and STD before truncation', mean_norm, std_norm)\n",
    "med_tr, mad_tr = np.median(train_data, axis=(0, 2, 3)), stats.median_absolute_deviation(train_data, axis=(0, 2, 3))\n",
    "print(\"Median and MAD before truncation\", med_tr, mad_tr)\n",
    "\n",
    "# NOTE: NEED TO put eval_data before train_data to avoid a bug wherein we modify train_data and median/mad\n",
    "eval_data = truncate_data_median(eval_data, med_tr, mad_tr)\n",
    "print(\"eval data truncated based on median\")\n",
    "train_data = truncate_data_median(train_data, med_tr, mad_tr)\n",
    "print(\"train data truncated based on median\")\n",
    "\n",
    "mean_norm, std_norm = np.mean(train_data, axis=(0, 2, 3)), np.std(train_data, axis=(0, 2, 3))\n",
    "print('Mean and STD before normalization', mean_norm, std_norm)\n",
    "med_tr, mad_tr = np.median(train_data, axis=(0, 2, 3)), stats.median_absolute_deviation(train_data, axis=(0, 2, 3))\n",
    "print(\"Median and MAD before normalization\", med_tr, mad_tr)\n",
    "\n",
    "# Per-lead normalization (there are 12 leads)\n",
    "assert (len(med_tr) == train_data.shape[1])\n",
    "for i in range(len(med_tr)):\n",
    "    tic = time.perf_counter()\n",
    "    print(\"pre-normalizing min:\",train_data[:, i, :, :].min())\n",
    "    print(\"pre-normalizing max:\", train_data[:, i, :, :].max())\n",
    "    eval_data[:, i, :, :] = (eval_data[:, i, :, :] - med_tr[i]) / mad_tr[i]\n",
    "    train_data[:, i, :, :] = (train_data[:, i, :, :] - med_tr[i]) / mad_tr[i]\n",
    "    print(\"post-normalizing min:\",train_data[:, i, :, :].min())\n",
    "    print(\"post-normalizing max:\", train_data[:, i, :, :].max())\n",
    "    toc = time.perf_counter()\n",
    "    print(f'Processed iter {i} in {toc - tic:0.2f} seconds')\n",
    "\n",
    "mean_norm, std_norm = np.mean(train_data, axis=(0, 2, 3)), np.std(train_data, axis=(0, 2, 3))\n",
    "print('Mean and STD after normalization', mean_norm, std_norm)\n",
    "med_norm, mad_norm = np.median(train_data, axis=(0, 2, 3)), stats.median_absolute_deviation(train_data, axis=(0, 2, 3))\n",
    "print(\"Median and MAD after normalization\", med_norm, mad_norm)\n",
    "\n",
    "# Transpose data for model\n",
    "X_train = np.transpose(train_data, axes=[0, 3, 2, 1])\n",
    "print('X_train shape for model:', X_train.shape)\n",
    "X_eval = np.transpose(eval_data, axes=[0, 3, 2, 1])\n",
    "print('X_eval shape for model:', X_eval.shape)\n",
    "assert X_train.shape[1:] == (1, 2500, 12), \"train is not X,1,2500,12\"\n",
    "assert X_eval.shape[1:] == (1, 2500, 12), \"eval is not X,1,2500,12\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved feature arrays\n"
     ]
    }
   ],
   "source": [
    "# Write out feature arrays\n",
    "np.save('/labs/amyloid/CardiacAmyloid/ValveNetExtraData/57k_train_df_pace_removed_2x_poor_quality_removed_any_patient_negative_as_features_5_sigma_robust_normalization.npy', X_train)\n",
    "np.save('/labs/amyloid/CardiacAmyloid/ValveNetExtraData/29k_eval_df_pace_removed_2x_poor_quality_removed_any_patient_negative_as_features_5_sigma_robust_normalization.npy', X_eval)\n",
    "print('Saved feature arrays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Commented out below is old approach pre 2/06/21\n",
    "# # Transpose for normalization\n",
    "# train_data = np.transpose(trainData, axes=[0, 2, 1, 3])\n",
    "# print('train_data norm_prep shape:', train_data.shape)\n",
    "# eval_data = np.transpose(evalData, axes=[0, 2, 1, 3])\n",
    "# print('eval_data norm_prep shape:', eval_data.shape)\n",
    "# assert train_data.shape[1:] == (12, 2500, 1), \"train is not X,12,2500,1\"\n",
    "# assert eval_data.shape[1:] == (12, 2500, 1), \"eval is not X,12,2500,1\"\n",
    "\n",
    "# sigma=5\n",
    "\n",
    "# def truncate_data_median(target_array, median, mad):\n",
    "#     datamadlimit = target_array\n",
    "#     assert (len(med_tr) == datamadlimit.shape[1])\n",
    "#     # Truncate data to sigma*mad to limit outlier effect and use robust statistical approach over mean\n",
    "#     for i in range(len(median)):\n",
    "#         print(\"pre-truncation min:\",datamadlimit[:, i, :, :].min())\n",
    "#         print(\"pre-truncation max:\", datamadlimit[:, i, :, :].max())\n",
    "#         datamadlimit[:, i, :, :] = np.where(datamadlimit[:, i, :, :] > median[i] + (sigma * mad[i]), median[i] + (sigma * mad), datamadlimit[:, i, :, :])\n",
    "#         datamadlimit[:, i, :, :] = np.where(datamadlimit[:, i, :, :] < median[i] - (sigma * mad[i]), median[i] - (sigma * mad), datamadlimit[:, i, :, :])\n",
    "#     for i in range(len(median)):\n",
    "#         print(\"post-truncation min:\",datamadlimit[:, i, :, :].min())\n",
    "#         print(\"post-truncation max:\", datamadlimit[:, i, :, :].max())\n",
    "#     # datanorm2 = cv2.normalize(datamadlimit, datamadlimit, -1, 1, cv2.NORM_MINMAX)\n",
    "#     return datamadlimit\n",
    "\n",
    "\n",
    "# # Compute mean and standard deviation for normalization\n",
    "# # median = np.median(train_data)\n",
    "# # mad = stats.median_absolute_deviation(train_data)\n",
    "# mean_norm, std_norm = np.mean(train_data, axis=(0, 2, 3)), np.std(train_data, axis=(0, 2, 3))\n",
    "# print('Mean and STD before normalization', mean_norm, std_norm)\n",
    "# med_tr, mad_tr = np.median(train_data, axis=(0, 2, 3)), stats.median_absolute_deviation(train_data, axis=(0, 2, 3))\n",
    "# print(\"Median and MAD before normalization\", med_tr, mad_tr)\n",
    "\n",
    "# # NOTE: NEED TO put eval_data before train_data to avoid a bug wherein we modify train_data and median/mad\n",
    "# eval_data = truncate_data_median(eval_data, med_tr, mad_tr)\n",
    "# print(\"eval data truncated based on median\")\n",
    "# train_data = truncate_data_median(train_data, med_tr, mad_tr)\n",
    "# print(\"train data truncated based on median\")\n",
    "\n",
    "# # Per-lead normalization (there are 12 leads)\n",
    "# assert (len(med_tr) == train_data.shape[1])\n",
    "# for i in range(len(med_tr)):\n",
    "#     tic = time.perf_counter()\n",
    "#     print(\"pre-normalizing min:\",train_data[:, i, :, :].min())\n",
    "#     print(\"pre-normalizing max:\", train_data[:, i, :, :].max())\n",
    "#     train_data[:, i, :, :] = (train_data[:, i, :, :] - med_tr[i]) / mad_tr[i]\n",
    "#     eval_data[:, i, :, :] = (eval_data[:, i, :, :] - med_tr[i]) / mad_tr[i]\n",
    "#     print(\"post-normalizing min:\",train_data[:, i, :, :].min())\n",
    "#     print(\"post-normalizing max:\", train_data[:, i, :, :].max())\n",
    "#     toc = time.perf_counter()\n",
    "#     print(f'Processed iter {i} in {toc - tic:0.2f} seconds')\n",
    "\n",
    "# mean_norm, std_norm = np.mean(train_data, axis=(0, 2, 3)), np.std(train_data, axis=(0, 2, 3))\n",
    "# print('Mean and STD after normalization', mean_norm, std_norm)\n",
    "# med_norm, mad_norm = np.median(train_data, axis=(0, 2, 3)), stats.median_absolute_deviation(train_data, axis=(0, 2, 3))\n",
    "# print(\"Median and MAD after normalization\", med_norm, mad_norm)\n",
    "\n",
    "# # Transpose data for model\n",
    "# X_train = np.transpose(train_data, axes=[0, 3, 2, 1])\n",
    "# print('X_train shape for model:', X_train.shape)\n",
    "# X_eval = np.transpose(eval_data, axes=[0, 3, 2, 1])\n",
    "# print('X_eval shape for model:', X_eval.shape)\n",
    "# assert X_train.shape[1:] == (1, 2500, 12), \"train is not X,1,2500,12\"\n",
    "# assert X_eval.shape[1:] == (1, 2500, 12), \"eval is not X,1,2500,12\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:porch] *",
   "language": "python",
   "name": "conda-env-porch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
